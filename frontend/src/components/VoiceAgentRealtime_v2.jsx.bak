/**
 * Real-Time Streaming Voice Agent Component
 * ==========================================
 * 
 * This component implements a fully real-time voice assistant that:
 * - Captures microphone audio and streams to backend via WebSocket
 * - Displays live transcription (partial and final)
 * - Shows AI responses with typing animation
 * - Plays AI-generated speech audio in real-time
 * - Tracks extracted structured data (name, income, credit score, loan amount)
 * - Displays loan eligibility results
 * 
 * Tech Stack:
 * - MediaRecorder API for audio capture
 * - WebSocket for bi-directional streaming
 * - Web Audio API for playing TTS audio chunks
 * - React hooks for state management
 * 
 * @author AI Development Assistant
 * @date November 2025
 */

import React, { useState, useEffect, useRef, useCallback } from 'react';
import { Phone, PhoneOff, Volume2, VolumeX } from 'lucide-react';

const VoiceAgentRealtime = () => {
  // Connection state
  const [isConnected, setIsConnected] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [isMuted, setIsMuted] = useState(false);
  const [error, setError] = useState(null);

  // Conversation state
  const [partialTranscript, setPartialTranscript] = useState('');
  const [finalTranscripts, setFinalTranscripts] = useState([]);
  const [currentAiToken, setCurrentAiToken] = useState('');

  // Structured data extracted by AI
  const [extractedData, setExtractedData] = useState({});
  const [eligibilityResult, setEligibilityResult] = useState(null);

  // Refs for persistent connections
  const wsRef = useRef(null);
  const mediaRecorderRef = useRef(null);
  const audioContextRef = useRef(null);
  const audioQueueRef = useRef([]);
  const isPlayingRef = useRef(false);

  /**
   * Queue and play audio chunks sequentially
   */
  const queueAudioChunk = useCallback((base64Audio) => {
    audioQueueRef.current.push(base64Audio);
    if (!isPlayingRef.current) {
      playNextAudioChunk();
    }
  }, []);

  const playNextAudioChunk = useCallback(async () => {
    if (audioQueueRef.current.length === 0) {
      isPlayingRef.current = false;
      return;
    }

    isPlayingRef.current = true;
    const base64Audio = audioQueueRef.current.shift();

    try {
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
      }

      const audioContext = audioContextRef.current;
      const binaryString = atob(base64Audio);
      const bytes = new Uint8Array(binaryString.length);
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i);
      }

      const audioBuffer = await audioContext.decodeAudioData(bytes.buffer);
      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(audioContext.destination);
      
      source.onended = () => {
        playNextAudioChunk();
      };

      source.start(0);
    } catch (err) {
      console.error('Failed to play audio chunk:', err);
      playNextAudioChunk();
    }
  }, []);

  /**
   * Handle incoming WebSocket messages
   */
  const handleWebSocketMessage = useCallback((message) => {
    const { type, data } = message;

    switch (type) {
      case 'partial_transcript':
        setPartialTranscript(data);
        break;

      case 'final_transcript':
        setFinalTranscripts(prev => [...prev, { role: 'user', text: data }]);
        setPartialTranscript('');
        break;

      case 'ai_token':
        setCurrentAiToken(prev => prev + data);
        break;

      case 'audio_chunk':
        queueAudioChunk(data);
        if (currentAiToken) {
          setFinalTranscripts(prev => [...prev, { role: 'assistant', text: currentAiToken }]);
          setCurrentAiToken('');
        }
        break;

      case 'structured_update':
        setExtractedData(prev => ({ ...prev, ...data }));
        break;

      case 'eligibility_result':
        setEligibilityResult(data);
        break;

      case 'error':
        setError(data);
        break;

      default:
        console.warn('Unknown message type:', type);
    }
  }, [currentAiToken, queueAudioChunk]);

  /**
   * Initialize WebSocket connection to backend
   */
  const connectWebSocket = useCallback(() => {
    const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
    const wsUrl = `${protocol}//${window.location.hostname}:8000/api/voice/stream`;

    try {
      const ws = new WebSocket(wsUrl);

      ws.onopen = () => {
        console.log('WebSocket connected');
        setIsConnected(true);
        setError(null);
      };

      ws.onmessage = (event) => {
        try {
          const message = JSON.parse(event.data);
          handleWebSocketMessage(message);
        } catch (err) {
          console.error('Failed to parse WebSocket message:', err);
        }
      };

      ws.onerror = (err) => {
        console.error('WebSocket error:', err);
        setError('Connection error. Please check backend is running.');
      };

      ws.onclose = () => {
        console.log('WebSocket closed');
        setIsConnected(false);
        setTimeout(() => {
          if (wsRef.current === ws) {
            connectWebSocket();
          }
        }, 3000); // Auto-reconnect after 3 seconds
      };

      wsRef.current = ws;
    } catch (err) {
      console.error('Failed to create WebSocket:', err);
      setError('Failed to connect to voice agent');
    }
  }, [handleWebSocketMessage]);

  /**
   * Queue and play audio chunks sequentially
   */
  const queueAudioChunk = (base64Audio) => {
    audioQueueRef.current.push(base64Audio);
    if (!isPlayingRef.current) {
      playNextAudioChunk();
    }
  };

  /**
   * Play next audio chunk from queue
   */
  const playNextAudioChunk = async () => {
    if (audioQueueRef.current.length === 0) {
      isPlayingRef.current = false;
      return;
    }

    isPlayingRef.current = true;
    const base64Audio = audioQueueRef.current.shift();

    try {
      // Initialize AudioContext if needed
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
      }

      const audioContext = audioContextRef.current;

      // Decode base64 to binary
      const binaryString = atob(base64Audio);
      const bytes = new Uint8Array(binaryString.length);
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i);
      }

      // Decode audio data
      const audioBuffer = await audioContext.decodeAudioData(bytes.buffer);

      // Create source and play
      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(audioContext.destination);
      
      source.onended = () => {
        playNextAudioChunk();
      };

      source.start(0);
    } catch (err) {
      console.error('Failed to play audio chunk:', err);
      playNextAudioChunk(); // Continue with next chunk
    }
  };

  /**
   * Start recording microphone audio
   */
  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 16000,
          echoCancellation: true,
          noiseSuppression: true,
        }
      });

      // Create MediaRecorder with appropriate format
      let mimeType = 'audio/webm;codecs=opus';
      if (!MediaRecorder.isTypeSupported(mimeType)) {
        mimeType = 'audio/webm';
      }

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType,
        audioBitsPerSecond: 16000
      });

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0 && wsRef.current?.readyState === WebSocket.OPEN) {
          // Send audio chunk to backend
          event.data.arrayBuffer().then(buffer => {
            wsRef.current.send(buffer);
          });
        }
      };

      mediaRecorder.onerror = (err) => {
        console.error('MediaRecorder error:', err);
        setError('Microphone error');
      };

      // Record in small chunks for low latency
      mediaRecorder.start(100); // 100ms chunks

      mediaRecorderRef.current = mediaRecorder;
      setIsRecording(true);
      setError(null);

    } catch (err) {
      console.error('Failed to start recording:', err);
      setError('Microphone access denied. Please allow microphone access.');
    }
  };

  /**
   * Stop recording microphone audio
   */
  const stopRecording = () => {
    if (mediaRecorderRef.current) {
      mediaRecorderRef.current.stop();
      mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
      mediaRecorderRef.current = null;
    }
    setIsRecording(false);
  };

  /**
   * Toggle mute/unmute
   */
  const toggleMute = () => {
    if (audioContextRef.current) {
      if (isMuted) {
        audioContextRef.current.resume();
      } else {
        audioContextRef.current.suspend();
      }
      setIsMuted(!isMuted);
    }
  };

  /**
   * Connect on mount, disconnect on unmount
   */
  useEffect(() => {
    connectWebSocket();

    return () => {
      if (wsRef.current) {
        wsRef.current.close();
      }
      stopRecording();
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
    };
  }, [connectWebSocket]);

  /**
   * Handle start/stop call button
   */
  const handleCallToggle = () => {
    if (isRecording) {
      stopRecording();
    } else if (isConnected) {
      startRecording();
    }
  };

  return (
    <div className="flex flex-col h-full max-w-4xl mx-auto p-6 bg-gradient-to-br from-blue-50 to-indigo-50">
      {/* Header */}
      <div className="text-center mb-6">
        <h1 className="text-3xl font-bold text-gray-800 mb-2">
          üéôÔ∏è LoanVoice Agent
        </h1>
        <p className="text-gray-600">
          Real-time AI voice assistant for loan eligibility
        </p>
      </div>

      {/* Connection Status */}
      <div className="flex items-center justify-center gap-4 mb-6">
        <div className={`px-4 py-2 rounded-full ${
          isConnected ? 'bg-green-100 text-green-700' : 'bg-red-100 text-red-700'
        }`}>
          <span className="font-medium">
            {isConnected ? 'üü¢ Connected' : 'üî¥ Disconnected'}
          </span>
        </div>

        {isRecording && (
          <div className="px-4 py-2 bg-blue-100 text-blue-700 rounded-full animate-pulse">
            <span className="font-medium">üé§ Listening...</span>
          </div>
        )}
      </div>

      {/* Error Display */}
      {error && (
        <div className="bg-red-100 border border-red-400 text-red-700 px-4 py-3 rounded mb-4">
          <p className="font-medium">‚ö†Ô∏è {error}</p>
        </div>
      )}

      {/* Extracted Data Display */}
      {Object.keys(extractedData).length > 0 && (
        <div className="bg-white rounded-lg shadow-md p-4 mb-4">
          <h3 className="font-bold text-gray-700 mb-3">üìã Collected Information</h3>
          <div className="grid grid-cols-2 gap-3">
            {extractedData.name && (
              <div className="bg-blue-50 p-3 rounded">
                <span className="text-sm text-gray-600">Name</span>
                <p className="font-semibold">{extractedData.name}</p>
              </div>
            )}
            {extractedData.monthly_income && (
              <div className="bg-green-50 p-3 rounded">
                <span className="text-sm text-gray-600">Monthly Income</span>
                <p className="font-semibold">${extractedData.monthly_income.toLocaleString()}</p>
              </div>
            )}
            {extractedData.credit_score && (
              <div className="bg-purple-50 p-3 rounded">
                <span className="text-sm text-gray-600">Credit Score</span>
                <p className="font-semibold">{extractedData.credit_score}</p>
              </div>
            )}
            {extractedData.loan_amount && (
              <div className="bg-yellow-50 p-3 rounded">
                <span className="text-sm text-gray-600">Loan Amount</span>
                <p className="font-semibold">${extractedData.loan_amount.toLocaleString()}</p>
              </div>
            )}
          </div>
        </div>
      )}

      {/* Eligibility Result */}
      {eligibilityResult && (
        <div className={`rounded-lg shadow-lg p-6 mb-4 ${
          eligibilityResult.approved ? 'bg-green-100 border-2 border-green-500' : 'bg-red-100 border-2 border-red-500'
        }`}>
          <h3 className="text-2xl font-bold mb-2">
            {eligibilityResult.approved ? '‚úÖ Loan Approved!' : '‚ùå Loan Denied'}
          </h3>
          <p className="text-lg mb-2">
            Approval Probability: <span className="font-bold">{(eligibilityResult.probability * 100).toFixed(1)}%</span>
          </p>
          <p className="text-sm text-gray-700">
            Confidence: {(eligibilityResult.confidence * 100).toFixed(1)}%
          </p>
        </div>
      )}

      {/* Conversation Display */}
      <div className="flex-1 bg-white rounded-lg shadow-md p-4 mb-4 overflow-y-auto">
        <div className="space-y-3">
          {finalTranscripts.map((msg, idx) => (
            <div
              key={idx}
              className={`flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'}`}
            >
              <div
                className={`max-w-xs px-4 py-2 rounded-lg ${
                  msg.role === 'user'
                    ? 'bg-blue-500 text-white'
                    : 'bg-gray-200 text-gray-800'
                }`}
              >
                <p>{msg.text}</p>
              </div>
            </div>
          ))}

          {/* Current AI Response (typing effect) */}
          {currentAiToken && (
            <div className="flex justify-start">
              <div className="max-w-xs px-4 py-2 rounded-lg bg-gray-200 text-gray-800">
                <p className="animate-pulse">{currentAiToken}</p>
              </div>
            </div>
          )}

          {/* Partial Transcript (user still speaking) */}
          {partialTranscript && (
            <div className="flex justify-end">
              <div className="max-w-xs px-4 py-2 rounded-lg bg-blue-300 text-blue-900 opacity-70">
                <p className="italic">{partialTranscript}...</p>
              </div>
            </div>
          )}
        </div>
      </div>

      {/* Control Buttons */}
      <div className="flex items-center justify-center gap-4">
        {/* Call Button */}
        <button
          onClick={handleCallToggle}
          disabled={!isConnected}
          className={`p-6 rounded-full shadow-lg transition-all transform hover:scale-110 ${
            isRecording
              ? 'bg-red-500 hover:bg-red-600'
              : 'bg-green-500 hover:bg-green-600'
          } ${!isConnected ? 'opacity-50 cursor-not-allowed' : ''}`}
        >
          {isRecording ? (
            <PhoneOff className="w-8 h-8 text-white" />
          ) : (
            <Phone className="w-8 h-8 text-white" />
          )}
        </button>

        {/* Mute Button */}
        <button
          onClick={toggleMute}
          disabled={!isRecording}
          className={`p-4 rounded-full shadow-md transition-all ${
            isMuted ? 'bg-gray-400' : 'bg-blue-500 hover:bg-blue-600'
          } ${!isRecording ? 'opacity-50 cursor-not-allowed' : ''}`}
        >
          {isMuted ? (
            <VolumeX className="w-6 h-6 text-white" />
          ) : (
            <Volume2 className="w-6 h-6 text-white" />
          )}
        </button>
      </div>

      {/* Help Text */}
      <div className="mt-6 text-center text-sm text-gray-600">
        <p>Click the green phone button to start talking with the AI assistant</p>
        <p className="mt-1">The agent will help assess your loan eligibility</p>
      </div>
    </div>
  );
};

export default VoiceAgentRealtime;
